# HFT Forecast – Model Training Repository

This repository contains the **model training code** for the HFT forecasting pipeline.

⚠️ **Large market data (SLIM Parquet / ZIP files) is NOT stored in GitHub.**  
All multi-GB data is stored in **Cloudflare R2 (S3-compatible object storage)** and downloaded locally using scripts in this repo.

---

# Design Principles

- GitHub → code, scripts, documentation
- GitHub → no large data, no trained models
- Cloudflare R2 → SLIM Parquet & ZIP data (2–3 GB per file)
- Deterministic local file structure for training
- Date-scoped model outputs for reproducibility

---

# Expected Local Directory Structure

The training code assumes the following structure exists locally:

```
~/HFT_forecast/
├── model_data/
│   └── <yyyy>/
│       └── daily/
│           ├── live_YYYYMMDD_ES*_SLIM.parquet
│           ├── live_YYYYMMDD_ZN*_SLIM.parquet
│           └── ...
│
├── fit_data/
│   └── roll30/
│       └── live/
│           └── <startdate>_<enddate>/
│               ├── model.pt
│               ├── model_config.json
│               └── metrics.json
│
├── logs_hft/
│   └── train/
│       ├── train_YYYYMMDD.log
│       └── ...
│
├── scripts/
│   └── r2_download.py
│
├── HFT_LSTM_train_POST30_ms_torch_multitask_b.py
└── README.md

```
# Input Data

## Required input path
~/HFT_forecast/model_data/<yyyy>/daily/*.SLIM.parquet

- One file per symbol per trading day
- Generated by the live SLIM / Databento pipeline
- Data must be downloaded from Cloudflare R2 before training

# Model Outputs

## Output model path

```
~/HFT_forecast/fit_data/roll30/live/<startdate>_<enddate>/
```
## Each training run writes:
- .pt : trained PyTorch model
-	.json : model configuration / metadata
-	Optional summary CSVs and metrics

This structure allows multiple rolling windows to coexist cleanly.

# Training Logs

## Log path

```
~/HFT_forecast/logs_hft/train/*.log
```
- One or more logs per training run
- Includes data range, hyperparameters, and training diagnostics

# Training Script

## Primary training entry point

```
~/HFT_forecast/HFT_LSTM_train_POST30_ms_torch_multitask_b.py
```

## This script:
- Reads SLIM Parquet files from model_data/<yyyy>/daily/
- Trains POST30 multi-task LSTM models
- Writes outputs to fit_data/roll30/live/<startdate>_<enddate>/
- Emits logs to logs_hft/train/

# Cloudflare R2 (Large Data Storage)

## Download Script

```
scripts/r2_download.py
```

All download logic lives in:
The script:
- Connects to Cloudflare R2 using the S3 API
- Downloads multi-GB objects safely (multipart transfer)
- Writes files into:

```
~/HFT_forecast/model_data/<yyyy>/daily/
```

# Running Model Training (After Data Download)

Once the required data exists under:

```
~/HFT_forecast/model_data/<yyyy>/daily/
```

Training Command (Multi-Instrument, Multi-GPU)

```bash
PY=${PY:-python3}

LOGDIR="/home/ubuntu/HFT_forecast/logs_hft/train"
SAVE_ROOT="/home/ubuntu/HFT_forecast/fit_data/roll30/live"
DATA_2025="/home/ubuntu/HFT_forecast/model_data/2025/daily"
DATA_2026="/home/ubuntu/HFT_forecast/model_data/2026/daily"

export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

PROFILE="hft"

INSTRUMENTS=(ES ZN)
GPUS=(0 1)

for idx in "${!INSTRUMENTS[@]}"; do
  INSTRUMENT="${INSTRUMENTS[$idx]}"
  GPU="${GPUS[$((idx % ${#GPUS[@]}))]}"

  # Defaults (ES)
  ROLLING_DAYS=30
  EPOCHS=4
  STEP_MS=3000
  SAMPLE_SIZE=600000
  BATCH_ANCHORS=288
  LOGTAG="roll30"

  # Overrides (ZN)
  if [[ "$INSTRUMENT" == "ZN" ]]; then
    ROLLING_DAYS=60
    EPOCHS=5
    STEP_MS=5000
    SAMPLE_SIZE=900000
    BATCH_ANCHORS=384
    LOGTAG="roll60"
  fi

  export CUDA_VISIBLE_DEVICES="$GPU"

  echo "[INFO] Launching ${INSTRUMENT}/${PROFILE} on GPU ${GPU}"

  nohup "$PY" -u /home/ubuntu/HFT_forecast/HFT_LSTM_train_POST30_ms_torch_multitask_b.py \
    --data_roots "$DATA_2025" "$DATA_2026" \
    --event_type live \
    --profile "$PROFILE" \
    --instrument "$INSTRUMENT" \
    --parent_symbol "$INSTRUMENT.FUT" \
    --book_features auto \
    --book_max_level 1 \
    --save_dir "$SAVE_ROOT" \
    --summary_csv "$SAVE_ROOT/summary_${LOGTAG}_live_${INSTRUMENT}_${PROFILE}_L1.csv" \
    --rolling_days "$ROLLING_DAYS" \
    --exclude_today \
    --accept_daily_single \
    --epochs "$EPOCHS" \
    --batch_size 288 \
    --win_a 0 \
    --win_b 90000 \
    --batch_anchors "$BATCH_ANCHORS" \
    --sample_size "$SAMPLE_SIZE" \
    --step_ms "$STEP_MS" \
    --arch lstm \
    --lstm_hidden 64 \
    --lstm_layers 2 \
    --lstm_dropout 0.2 \
    --target logret \
    --y_scale 1.0 \
    --lam_reg 1.0 \
    > "$LOGDIR/train_${LOGTAG}_live_${INSTRUMENT}_${PROFILE}_L1_gpu${GPU}_$(date -u +%Y%m%d_%H%M%S).log" 2>&1 &

done

```

# Monitoring

Check running jobs:

```bash
tail -f ~/HFT_forecast/logs_hft/train/*.log
```

# End-to-End Workflow Summary
	1.	Download data from Cloudflare R2
	2.	Verify data exists under model_data/<yyyy>/daily/
	3.	Launch training command
	4.	Monitor logs in logs_hft/train/
	5.	Retrieve trained models from fit_data/roll30/live/<startdate>_<enddate>/

# Summary
- Input data
  model_data/<yyyy>/daily/*.SLIM.parquet
- Training script
  HFT_LSTM_train_POST30_ms_torch_multitask_b.py
- Model outputs
  fit_data/roll30/live/<startdate>_<enddate>/
- Logs
  logs_hft/train/*.log
- Large data storage
  Cloudflare R2 (S3-compatible)
